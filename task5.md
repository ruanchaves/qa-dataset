Anthropic just dropped a fascinating deep-dive on measuring political bias in AI models. Here's what caught my attention: they used Claude Sonnet 4.5 as an automated grader and found it was *more consistent* than human raters.

They tested 1,350 prompt pairs, measuring three dimensions: even-handedness, acknowledgment of opposing perspectives, and refusal rates. 

What I'm curious about: how does their "Paired Prompts" approach compare to what we've been exploring? And what do we think about using models as graders for bias evaluation: is that circular reasoning or a pragmatic solution?

Worth a read: [link to article]. Would love to hear thoughts on the methodology and whether we should consider adapting any of their approaches.